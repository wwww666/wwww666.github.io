---
layout:     post
title:      HDFS读写流程
subtitle:    HDFS
date:       2021-03-13
author:     wzk
header-img: img/home-bg-o.jpg
catalog: true
tags:
    - HDFS
---

## 前言

每日学习总结记录



>关键词：HDFS

### HDFS读写流程
目前在准备大数据研发方面的招聘，所以记录一下复习内容；
HDFS的写流程是怎么执行的呢：
1.client跟namenode通信请求上传文件，namenode会检查该文件是否存在，检查父母目录是否存在。
2.namenode返回是否可以上传。
3.client继续向namenode请求第一个block该传输到哪台datanode服务器上。
4.namenode返回3台datanode机器ABC。
5.client请求第一台机器A上传数据，A收到请求会继续调用B，然后B再调用C，整个请求调用完成之后逐级返回给client。
6.client开始向A传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给
B，B再传给C，A每传一个packet就会放入一个应答队列等待回应
7.当一个block传输完成之后，client再次请求namenode上传第二个block，以此类推依次将数据写入HDFS中。
HDFS的读流程又是怎么执行的呢：
1.首先client与namenode通信查询元数据，找到文件块所在的datanode机器。
2.就近原则，随机挑选一台datanode机器，请求建立socket流。
3.datanode开始发送数据，从磁盘里读取数据放入流，以packet为单位来校验
4.client以packet为单位接收，先在本地缓存，然后写入目标文件。

这里补充一个小知识：
上传数据一般HDFS默认备份三份，也就是分散到三台datanode机器，那么三台机器是如何选择的，它的策略是这样的：
1.第一个副本考虑跟client最接近的机器（同一机架）
2.第二个副本存放到第一个副本相同机架的其他区域随机选择一台
3.第三个副本存放到其他机架随机选择的一台机器上，增加数据的可靠性




 

